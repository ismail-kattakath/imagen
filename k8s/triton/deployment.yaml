# =============================================================================
# TRITON INFERENCE SERVER DEPLOYMENT
# =============================================================================
#
# Centralized GPU inference server with automatic batching.
# All thin workers call Triton via gRPC instead of loading models directly.
#
# Benefits:
# - Single GPU serves all 5 model types
# - Automatic dynamic batching
# - Multi-model scheduling
# - ~4x throughput improvement
#
# =============================================================================

apiVersion: apps/v1
kind: Deployment
metadata:
  name: triton-inference-server
  namespace: imagen
  labels:
    app: triton
    app.kubernetes.io/component: inference
    app.kubernetes.io/part-of: imagen
spec:
  replicas: 1
  selector:
    matchLabels:
      app: triton
  template:
    metadata:
      labels:
        app: triton
        app.kubernetes.io/component: inference
        app.kubernetes.io/part-of: imagen
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8002"
        prometheus.io/path: "/metrics"
    spec:
      serviceAccountName: imagen-worker
      
      # GPU node selection
      nodeSelector:
        cloud.google.com/gke-accelerator: nvidia-tesla-t4
      
      tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
      
      containers:
        - name: triton
          # Custom Triton image with model repository embedded
          image: us-central1-docker.pkg.dev/PROJECT_ID/imagen/triton:latest
          
          # Model repository is embedded in image at /models/repository
          # HuggingFace cache uses PVC at /models/huggingface
          args:
            - tritonserver
            - --model-repository=/models/repository
            - --log-verbose=1
            - --log-info=true
            - --log-warning=true
            - --log-error=true
            - --metrics-port=8002
            - --allow-metrics=true
            - --strict-model-config=false
            - --cuda-memory-pool-byte-size=0:4294967296  # 4GB CUDA pool
          
          ports:
            - name: http
              containerPort: 8000
              protocol: TCP
            - name: grpc
              containerPort: 8001
              protocol: TCP
            - name: metrics
              containerPort: 8002
              protocol: TCP
          
          resources:
            limits:
              nvidia.com/gpu: 1
              memory: "24Gi"
            requests:
              memory: "16Gi"
              cpu: "4"
          
          # Health checks
          livenessProbe:
            httpGet:
              path: /v2/health/live
              port: http
            initialDelaySeconds: 60
            periodSeconds: 30
            timeoutSeconds: 10
            failureThreshold: 3
          
          readinessProbe:
            httpGet:
              path: /v2/health/ready
              port: http
            initialDelaySeconds: 120
            periodSeconds: 10
            timeoutSeconds: 10
            failureThreshold: 3
          
          # Startup probe - models take time to load
          startupProbe:
            httpGet:
              path: /v2/health/ready
              port: http
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 10
            failureThreshold: 30  # 5 minutes max startup
          
          env:
            - name: HF_HOME
              value: "/models/huggingface"
            - name: TRANSFORMERS_CACHE
              value: "/models/huggingface"
            - name: CUDA_VISIBLE_DEVICES
              value: "0"
          
          volumeMounts:
            # HuggingFace model cache (downloaded at runtime)
            - name: models
              mountPath: /models/huggingface
            # Shared memory for PyTorch DataLoader
            - name: shm
              mountPath: /dev/shm
      
      volumes:
        # Persistent model cache storage
        - name: models
          persistentVolumeClaim:
            claimName: models-pvc
        # Shared memory for PyTorch
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: 2Gi

---
# =============================================================================
# TRITON SERVICE
# =============================================================================

apiVersion: v1
kind: Service
metadata:
  name: triton
  namespace: imagen
  labels:
    app: triton
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 8000
      targetPort: 8000
      protocol: TCP
    - name: grpc
      port: 8001
      targetPort: 8001
      protocol: TCP
    - name: metrics
      port: 8002
      targetPort: 8002
      protocol: TCP
  selector:
    app: triton

---
# =============================================================================
# TRITON HPA
# =============================================================================
#
# Scales Triton based on inference queue latency.
# For most workloads, 1-2 Triton instances is sufficient.
#
# =============================================================================

apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: triton-hpa
  namespace: imagen
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: triton-inference-server
  minReplicas: 1
  maxReplicas: 3
  metrics:
    # Scale based on GPU utilization via DCGM metrics (if available)
    # Otherwise, fall back to CPU as proxy
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 120
      policies:
        - type: Pods
          value: 1
          periodSeconds: 120
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        - type: Pods
          value: 1
          periodSeconds: 300
