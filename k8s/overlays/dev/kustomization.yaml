# =============================================================================
# DEVELOPMENT OVERLAY - TRITON ARCHITECTURE
# =============================================================================
#
# Usage:
#   kubectl apply -k k8s/overlays/dev
#
# Before using, update the values below:
#   1. Replace YOUR_PROJECT_ID with your GCP project ID
#   2. Optionally adjust replica counts, resource limits, etc.
#
# =============================================================================

apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

# Reference the base configuration
resources:
  - ../../

# Replace image names
images:
  - name: us-central1-docker.pkg.dev/PROJECT_ID/imagen/worker
    newName: us-central1-docker.pkg.dev/YOUR_PROJECT_ID/imagen/worker
    newTag: latest
  - name: us-central1-docker.pkg.dev/PROJECT_ID/imagen/triton
    newName: us-central1-docker.pkg.dev/YOUR_PROJECT_ID/imagen/triton
    newTag: latest

# Override ConfigMap with dev values
configMapGenerator:
  - name: imagen-config
    behavior: replace
    literals:
      - GOOGLE_CLOUD_PROJECT=YOUR_PROJECT_ID
      - GCS_BUCKET=YOUR_PROJECT_ID-imagen-images
      - MODEL_CACHE_DIR=/models
      - DEVICE=cuda
      - TORCH_DTYPE=float16
      - TRITON_URL=triton:8001

# Dev-specific patches
patches:
  # Scale down Triton for dev (single replica)
  - patch: |-
      - op: replace
        path: /spec/minReplicas
        value: 1
      - op: replace
        path: /spec/maxReplicas
        value: 2
    target:
      kind: HorizontalPodAutoscaler
      name: triton-hpa
  
  # Scale down workers for dev
  - patch: |-
      - op: replace
        path: /spec/minReplicas
        value: 0
      - op: replace
        path: /spec/maxReplicas
        value: 3
    target:
      kind: HorizontalPodAutoscaler
      name: ".*-worker-hpa"
  
  # Reduce Triton resources for dev
  - patch: |-
      - op: replace
        path: /spec/template/spec/containers/0/resources/requests/memory
        value: "8Gi"
      - op: replace
        path: /spec/template/spec/containers/0/resources/limits/memory
        value: "16Gi"
    target:
      kind: Deployment
      name: triton-inference-server
